cat("  Skattad varians    =", var(sample1))
cat("Sample 2 (n=100, p=0.1):\n")
cat("  Skattat medelvärde =", mean(sample2))
cat("  Skattad varians    =", var(sample2))
# Simulate 10000 samples from bin(n=100, p=0.5) and plot a histogram.
set.seed(123)  # för reproducerbarhet
sample1 <- rbinom(n = 100, size = 100, prob = 0.5)
# Then simulate 10000 samples from bin(n= 100, p = 0.1) and plot a histogram.
sample2 <- rbinom(n = 100, size = 100, prob = 0.1)
# Set up plotting area: 1 row, 2 columns
par(mfrow = c(1, 2))
hist(sample1, main = "Bin(100, 0.5)", col = "lightblue")
hist(sample2, main = "Bin(100, 0.1)", col = "lightpink")
# In the plots, what is the "middle" of the plot?
# - Förväntat värde:     E(X) = n * p
#"mitten" i histogrammet (toppunkt) bör ligga runt 50 (100*0,5) respektive 10 (100*0,1).
# Which plot has a "wider" distribution?
# Varians i en binomialfördelning:
# Om X ~ Bin(n, p) så är:
# Varians(X) = n * p * (1 - p)
# Var sample1 = 100⋅0.5⋅0.5=25 #större spridning
# Var sample2 = 100⋅0.1⋅0.9=9
# Teoretiskt förväntat värde och varians för binomialfördelning:
# Om X ~ Bin(n, p) så gäller:
# - Förväntat värde:     E(X) = n * p
# - Varians:             Var(X) = n * p * (1 - p)
# För Bin(n = 100, p = 0.5):
# - E(X) = 100 * 0.5 = 50
# - Var(X) = 100 * 0.5 * (1 - 0.5) = 25
# För Bin(n = 100, p = 0.1):
# - E(X) = 100 * 0.1 = 10
# - Var(X) = 100 * 0.1 * (1 - 0.1) = 9
# Estimate the mean and variance of the two samples that you simulated by using
# the functions mean() and var().
# Skriv ut resultaten
cat("Sample 1 (n=100, p=0.5):\n")
cat("  Skattat medelvärde =", mean(sample1))
cat("  Skattad varians    =", var(sample1))
cat("Sample 2 (n=100, p=0.1):\n")
cat("  Skattat medelvärde =", mean(sample2))
cat("  Skattad varians    =", var(sample2))
# Simulate 10000 samples from bin(n=100, p=0.5) and plot a histogram.
set.seed(123)  # för reproducerbarhet
sample1 <- rbinom(n = 10, size = 100, prob = 0.5)
# Then simulate 10000 samples from bin(n= 100, p = 0.1) and plot a histogram.
sample2 <- rbinom(n = 10, size = 100, prob = 0.1)
# Set up plotting area: 1 row, 2 columns
par(mfrow = c(1, 2))
hist(sample1, main = "Bin(100, 0.5)", col = "lightblue")
hist(sample2, main = "Bin(100, 0.1)", col = "lightpink")
# In the plots, what is the "middle" of the plot?
# - Förväntat värde:     E(X) = n * p
#"mitten" i histogrammet (toppunkt) bör ligga runt 50 (100*0,5) respektive 10 (100*0,1).
# Which plot has a "wider" distribution?
# Varians i en binomialfördelning:
# Om X ~ Bin(n, p) så är:
# Varians(X) = n * p * (1 - p)
# Var sample1 = 100⋅0.5⋅0.5=25 #större spridning
# Var sample2 = 100⋅0.1⋅0.9=9
# Teoretiskt förväntat värde och varians för binomialfördelning:
# Om X ~ Bin(n, p) så gäller:
# - Förväntat värde:     E(X) = n * p
# - Varians:             Var(X) = n * p * (1 - p)
# För Bin(n = 100, p = 0.5):
# - E(X) = 100 * 0.5 = 50
# - Var(X) = 100 * 0.5 * (1 - 0.5) = 25
# För Bin(n = 100, p = 0.1):
# - E(X) = 100 * 0.1 = 10
# - Var(X) = 100 * 0.1 * (1 - 0.1) = 9
# Estimate the mean and variance of the two samples that you simulated by using
# the functions mean() and var().
# Skriv ut resultaten
cat("Sample 1 (n=100, p=0.5):\n")
cat("  Skattat medelvärde =", mean(sample1))
cat("  Skattad varians    =", var(sample1))
cat("Sample 2 (n=100, p=0.1):\n")
cat("  Skattat medelvärde =", mean(sample2))
cat("  Skattad varians    =", var(sample2))
# Confidence Intervals ----------------------------------------------------
# Explain the code/results you see below
# -------------------------------------------------------------------------
help(t.test)
data <- rnorm(4322, mean = 5, sd = 4)
t.test(data, conf.level = 0.95)$conf.int
sd(data)
# Skapa ett stickprov
set.seed(123)
data <- rnorm(50, mean = 5, sd = 2)
# Beräkna medelvärde och 95 % konfidensintervall
t_result <- t.test(data, conf.level = 0.95)
mean_val <- mean(data)
ci <- t_result$conf.int  # lower and upper limit
# Rita grafen
hist(data, breaks = 10, col = "lightgray", main = "Konfidensintervall för medelvärde",
xlab = "Värde", xlim = range(ci) + c(-1, 1))
# Rita medelvärdet
abline(v = mean_val, col = "red", lwd = 2)
# Rita konfidensintervallets gränser
abline(v = ci[1], col = "blue", lty = 2, lwd = 2)  # nedre gräns
abline(v = ci[2], col = "blue", lty = 2, lwd = 2)  # övre gräns
# Lägg till en förklaring
legend("topright", legend = c("Medelvärde", "95% KI-gränser"),
col = c("red", "blue"), lty = c(1, 2), lwd = 2)
# Skapa ett stickprov
set.seed(123)
data <- rnorm(50, mean = 5, sd = 2)
# Beräkna medelvärde och 95 % konfidensintervall
t_result <- t.test(data, conf.level = 0.95)
mean_val <- mean(data)
ci <- t_result$conf.int  # lower and upper limit
# Rita grafen
hist(data, breaks = 5, col = "lightgray", main = "Konfidensintervall för medelvärde",
xlab = "Värde", xlim = range(ci) + c(-1, 1))
# Rita medelvärdet
abline(v = mean_val, col = "red", lwd = 2)
# Rita konfidensintervallets gränser
abline(v = ci[1], col = "blue", lty = 2, lwd = 2)  # nedre gräns
abline(v = ci[2], col = "blue", lty = 2, lwd = 2)  # övre gräns
# Lägg till en förklaring
legend("topright", legend = c("Medelvärde", "95% KI-gränser"),
col = c("red", "blue"), lty = c(1, 2), lwd = 2)
# Skapa ett stickprov
set.seed(123)
data <- rnorm(50, mean = 5, sd = 2)
# Beräkna medelvärde och 95 % konfidensintervall
t_result <- t.test(data, conf.level = 0.95)
mean_val <- mean(data)
ci <- t_result$conf.int  # lower and upper limit
# Rita grafen
hist(data, breaks = 5, col = "lightgray", main = "Konfidensintervall för medelvärde",
xlab = "Värde", xlim = range(ci) + c(-1, 1))
# Rita medelvärdet
abline(v = mean_val, col = "red", lwd = 2)
# Rita konfidensintervallets gränser
abline(v = ci[1], col = "blue", lty = 2, lwd = 2)  # nedre gräns
abline(v = ci[2], col = "blue", lty = 2, lwd = 2)  # övre gräns
# Lägg till en förklaring
legend("topright", legend = c("Medelvärde", "95% KI-gränser"),
col = c("red", "blue"), lty = c(1, 2), lwd = 2)
# Simulera data
set.seed(123)
x <- rnorm(100, mean = 0, sd = 4)
# T-tester
test_mu0 <- t.test(x, mu = 0)
test_mu5 <- t.test(x, mu = 5)
# Visa p-värden
cat("p-värde (mu = 0):", test_mu0$p.value, "\n")
cat("p-värde (mu = 5):", test_mu5$p.value, "\n")
# Plot t-fördelning
df <- length(x) - 1
curve(dt(x, df), from = -5, to = 5, lwd = 2, ylab = "Täthet",
main = "Visualisering av t-test", col = "gray20")
# Kritiska gränser vid 95% signifikansnivå
crit <- qt(0.975, df)
abline(v = c(-crit, crit), col = "red", lty = 2)
text(-crit, 0.02, "kritisk gräns", pos = 3, col = "red")
text(crit, 0.02, "kritisk gräns", pos = 3, col = "red")
# Lägg till observerade t-värden
abline(v = test_mu0$statistic, col = "blue", lty = 3, lwd = 2)
abline(v = test_mu5$statistic, col = "darkgreen", lty = 3, lwd = 2)
legend("topright", legend = c("t-fördelning", "t (mu = 0)", "t (mu = 5)", "kritiska gränser"),
col = c("gray20", "blue", "darkgreen", "red"), lty = c(1, 3, 3, 2), lwd = 2)
t.test(x, mu = 0)
# Testing for normality ---------------------------------------------------
# The Shapiro wilk normality test is a test of normality, the null
# hypothesis is that the data comes from a normal distribution.
# Comment on the results below.
# -------------------------------------------------------------------------
?shapiro.test
?shapiro.test #testar om datan i x kommer från en normalfördelning.
#Om p > 0.05 → Vi kan inte förkasta H₀ → Datan är kompatibel med normalfördelning
x1 <- rnorm(741, mean = 8, sd = 30)
x2 <- rpois(93, lambda = 3)
x3 <- rpois(301, lambda = 70)
shapiro.test(x1)
shapiro.test(x2)
shapiro.test(x3)
qqnorm(x1)
qqline(x1)
shapiro.test(x2)
qqnorm(x2)
qqline(x2)
qqline(x3)
qqline(x2)
qqline(x1)
qqline(x2)
qqnorm(x2)
qqnorm(x3)
qqnorm(x1)
qqnorm(x2)
# 1. 10 observationer
set.seed(1)
x10 <- rnorm(10, mean = 5, sd = 10)
mean(x10)  # skattat medelvärde
sd(x10)    # skattad standardavvikelse
# 2. 100 observationer
x100 <- rnorm(100, mean = 5, sd = 10)
mean(x100)
sd(x100)
# 3. 10000 observationer
x10000 <- rnorm(10000, mean = 5, sd = 10)
mean(x10000)
sd(x10000)
mean(x10)  # skattat medelvärde
sd(x10)    # skattad standardavvikelse
mean(x100)
sd(x100)
mean(x10000)
sd(x10000)
empty_vec <- c()
for (i in seq(600)) {
throws <- sample(1:6, i, TRUE)  # Simulerar i tärningskast
relative_freq <- sum(throws == 3)/length(throws)  # Relativ frekvens för treor
empty_vec <- c(empty_vec, relative_freq)  # Sparar frekvensen
}
plot(empty_vec, type="l", ylab='Relative Frequency', xlab='Number of Throws', main='Illustrating LLN')
abline(h=1/6, col = 'red') #den teoretiska sannolikheten för att få en trea
p <- 0.042
n <- 1000
z <- 1.6449  # från z-tabell för 90 % konfidensnivå
s_p <- sqrt(p * (1 - p) / n)  # standard error
ci_lower <- p - z * s_p
ci_upper <- p + z * s_p
print(ci_lower)
print(ci_upper)
ci_lower <- p - z*s_p
ci_upper <- p + z*s_p
print(ci_lower)
print(ci_upper)
# Inställningar
set.seed(42)
true_p <- 0.042
n <- 1000
z <- 1.6449  # z-värde för 90 % konfidensintervall
num_sim <- 100
# Tomma vektorer
lower_bounds <- numeric(num_sim)
upper_bounds <- numeric(num_sim)
contains_true_p <- logical(num_sim)
# Simulera 100 stickprov
for (i in 1:num_sim) {
sample <- rbinom(1, n, true_p) / n
se <- sqrt(sample * (1 - sample) / n)
lower <- sample - z * se
upper <- sample + z * se
lower_bounds[i] <- lower
upper_bounds[i] <- upper
contains_true_p[i] <- (lower <= true_p & true_p <= upper)
}
# Plot
plot(NULL, xlim = c(0.03, 0.055), ylim = c(1, num_sim),
xlab = "Andel", ylab = "Simulering",
main = "100 konfidensintervall (90 %) för proportion")
abline(v = true_p, col = "blue", lty = 2)
for (i in 1:num_sim) {
col <- ifelse(contains_true_p[i], "green", "red")
segments(lower_bounds[i], i, upper_bounds[i], i, col = col)
}
legend("bottomright",
legend = c("Innehåller sant p", "Missar sant p", "Sant värde (p = 0.042)"),
col = c("green", "red", "blue"), lty = c(1, 1, 2))
cat("Antal intervall som täckte sant p:", sum(contains_true_p), "av", num_sim, "\n")
# -------------------------------------------------------------------------
# Recall that the poisson distribution is a discrete distribution which
# can take the values 0, 1, 2, 3, ... .
# From the plot below you see that the distribution is not the same as the
# normal distribution, since it for example is not symmetric and can only attain values >= 0.
# However, the lambda parameter changes the look of the distribution,
# feel free to experiment with it to see the effect.
# -------------------------------------------------------------------------
plot(dpois(x=0:50,lambda=3),type="b")
x1 <- rpois(100, lambda = 7)
x2 <- rpois(1000, lambda = 7)
x3 <- rpois(10000, lambda = 7)
x4 <- rpois(100000, lambda = 7)
x5 <- rpois(1000000, lambda = 7)
x6 <- rpois(10000000, lambda = 7)
x7 <- rpois(100000000, lambda = 7)
par(mfrow=c(3,3))
hist(x1)
hist(x2)
hist(x3)
hist(x4)
hist(x5)
hist(x6)
hist(x7)
ci_lower <- 10 - 1.96*3
ci_upper <- 10 + 1.96*3
print(ci_lower)
print(ci_upper)
x <- rnorm(100, mean = 10, sd = 3)
result <- c()
for (i in x) {
if (i >= ci_lower & i <= ci_upper) {
result <- c(result, 0)
}
else {
result <- c(result, 1)
}
}
# print(result)
rel_freq_of_exceptions <- mean(result)
print(rel_freq_of_exceptions)
ci_lower <- 10 - 1.96*3
ci_upper <- 10 + 1.96*3
print(ci_lower)
print(ci_upper)
x <- rnorm(1000, mean = 10, sd = 3)
result <- c()
for (i in x) {
if (i >= ci_lower & i <= ci_upper) {
result <- c(result, 0)
}
else {
result <- c(result, 1)
}
}
# print(result)
rel_freq_of_exceptions <- mean(result)
print(rel_freq_of_exceptions)
ci_lower <- 10 - 1.96*3
ci_upper <- 10 + 1.96*3
print(ci_lower)
print(ci_upper)
x <- rnorm(10000, mean = 10, sd = 3)
result <- c()
for (i in x) {
if (i >= ci_lower & i <= ci_upper) {
result <- c(result, 0)
}
else {
result <- c(result, 1)
}
}
# print(result)
rel_freq_of_exceptions <- mean(result)
print(rel_freq_of_exceptions)
x1 <- c()
x2 <- c()
for (i in 1:100){
x1 <- c(x1, mean(rnorm(80, mean = 10, sd = 5))) #Tar ett stickprov på 80 observationer från N(10, 5) → räknar medelvärdet → lägger in i x1
x2 <- c(x2, mean(rnorm(80, mean = 10, sd = 20)))
}
# Plotting two histograms in same plot:
# https://stackoverflow.com/questions/3541713/how-to-plot-two-histograms-together-in-r .
h1 <- hist(x1) # Med SD = 5
h2 <- hist(x2) # Med SD = 20
plot(h1, col=rgb(0,0,1,1/4), xlim=c(2,19))  # first histogram
plot(h2, col=rgb(1,0,0,1/4), xlim=c(2,19), add=T)  # second
x1 <- c()
x2 <- c()
for (i in 1:100){
x1 <- c(x1, mean(rnorm(10000, mean = 10, sd = 5)))
x2 <- c(x2, mean(rnorm(10000, mean = 10, sd = 20)))
}
h1 <- hist(x1)
h2 <- hist(x2)
# Plotting two histograms in same plot: https://stackoverflow.com/questions/3541713/how-to-plot-two-histograms-together-in-r .
plot(h1, col=rgb(0,0,1,1/4), xlim=c(2,19))  # first histogram
plot(h2, col=rgb(1,0,0,1/4), xlim=c(2,19), add=T)  # second
```{r}
library(tidyverse)
library(fivethirtyeight)
install.packages('fivethirtyeightdata', repos =
'https://fivethirtyeightdata.github.io/drat/', type = 'source')
library(tidyverse)
library(fivethirtyeight)
library(tidyverse)
library(fivethirtyeight)
# Tidy data - long and wide form
## Drinks
```{r}
?drinks
drinks |> View()
drinks_smaller <- drinks |>
filter(country %in% c("USA", "China", "Sweden", "Argentina")) |>
select(-total_litres_of_pure_alcohol) |>
rename(beer = beer_servings, wine = wine_servings, spirit = spirit_servings)
drinks_smaller
![](drinks.png)
```{r}
drinks_smaller |>
pivot_longer(
cols = beer:wine,
names_to = "type",
values_to = "servings"
) |>
ggplot(aes(x = country, y = servings, fill = type)) +
geom_col(position = "dodge")
world_bank_pop
```{r}
pop2 <- world_bank_pop |>
pivot_longer(
cols = `2000`:`2017`,
names_to = "year",
values_to = "value"
) |>
mutate(year = parse_number(year))
world_bank_pop
pop2 |> head(20)
pop3 <- pop2 |>
separate(indicator, c(NA, "area", "variable"))
pop3 |> View()
pop4 <- pop3 |>
pivot_wider(
names_from = variable,
values_from = value
)
pop4 |>
filter(country == "ABW") |>
ggplot(aes(x = year, y = TOTL, color = area)) +
geom_line()
billboard
(
week = parse_number(week)
)
billboard_long <- billboard |>
pivot_longer(
cols = starts_with("wk"),
names_to = "week",
values_to = "rank",
values_drop_na = TRUE
) |>
mutate(
week = parse_number(week)
)
billboard_long |> View()
billboard_long |>
ggplot(aes(x = week, y = rank, group = track)) +
geom_line(alpha = .25) +
scale_y_reverse()
library(tidyverse)
library(tidymodels)
library(broom)
library(corrr)
search()
search()
library(tidyverse)
library(tidyverse)
library(tidymodels)
library(broom)
library(corrr)
diamonds |> head()
diamonds |>
ggplot(aes(x = price)) +
geom_histogram(binwidth = 100)
diamonds |> filter(price < 2000) |>
ggplot(aes(x = price)) +
geom_histogram(binwidth = 10)
diamonds |>
ggplot(aes(x = price)) +
geom_histogram(binwidth = .01) +
scale_x_log10()
diamonds <- diamonds |>
mutate(price = log10(price))
diamonds <- diamonds |>
mutate(price = log10(price))
diamonds |> filter(if_any(everything(), is.na))
diamonds |>
filter(if_any(everything(), ~ . <= 0))
diamonds <- diamonds |>
filter(if_all(everything(), ~ . != 0))
diamonds |> filter(carat < 3) |>
ggplot(aes(x = carat)) +
geom_histogram(binwidth = .01)
# Remove outliers first!
diamonds_long <- diamonds |>
pivot_longer(
cols = x:z,
names_to = "dimension",
values_to = "size")
diamonds_long |> head()
# Remove outliers first!
diamonds_long <- diamonds |>
pivot_longer(
cols = x:z,
names_to = "dimension",
values_to = "size")
diamonds_long |> head()
diamonds_long |>
ggplot(aes(y = size)) +
geom_boxplot() +
facet_wrap(~dimension)
diamonds |> select(depth, x:z) |> filter(y > 30 | z > 30)
diamonds |> select(depth, x:z) |> filter(y > 30 | z > 30)
diamonds <- diamonds |> filter(if_all(y:z, ~ . < 30))
diamonds |> filter(duplicated(diamonds))
dia_dup <- diamonds |>
group_by(carat, cut, clarity, color, price) |>
mutate(id = cur_group_id(), id_size = n()) |>
ungroup() |>
relocate(id, id_size) |>
arrange(-id_size)
dia_dup |> distinct(id, id_size) |> arrange(-id_size)
dia_dup |> filter(id == 2244) |> View()
dia_no_dup <- diamonds |>
summarise(
avg_x = mean(x),
avg_y = mean(y),
avg_z = mean(z),
avg_depth = mean(depth),
avg_table = mean(table),
.by = c(carat, cut, color, clarity, price)
)
dia_no_dup
diamonds |>
ggplot(aes(x = x, y = y)) +
geom_point()
diamonds_model <- lm(price ~ carat + color + cut + clarity + x + z, data = diamonds) |> summary()
dia_no_dup_model <- lm(price ~ carat + color + cut + clarity + avg_x + avg_z, data = dia_no_dup) |> summary()
diamonds_model
dia_no_dup_model
