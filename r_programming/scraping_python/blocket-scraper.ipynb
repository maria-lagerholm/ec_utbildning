{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException, ElementNotInteractableException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update with your chromedriver path\n",
    "service = ChromeService(executable_path='/usr/local/bin/chromedriver')\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "driver.get(\"\"\"https://www.blocket.se/bilar/sok?filter=%7B\"key\"%3A\"modelYear\"%2C\"range\"%3A%7B\"start\"%3A\"2015\"%2C\"end\"%3A\"\"%7D%7D&filter=%7B\"key\"%3A\"sellerType\"%2C\"values\"%3A%5B\"Privat\"%5D%7D\"\"\")\n",
    "\n",
    "try:\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.frame_to_be_available_and_switch_to_it((By.CSS_SELECTOR, \"#sp_message_iframe_1253915\"))\n",
    "    )\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.element_to_be_clickable((By.CSS_SELECTOR, \"button.sp_choice_type_11\"))\n",
    "    ).click()\n",
    "finally:\n",
    "    driver.switch_to.default_content()\n",
    "\n",
    "input(\"Press Enter to quit...\")\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "StaleElementReferenceException",
     "evalue": "Message: stale element reference: stale element not found in the current frame\n  (Session info: chrome=125.0.6422.141); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#stale-element-reference-exception\nStacktrace:\n0   chromedriver                        0x0000000104e2a4c8 chromedriver + 4302024\n1   chromedriver                        0x0000000104e22e10 chromedriver + 4271632\n2   chromedriver                        0x0000000104a5419c chromedriver + 278940\n3   chromedriver                        0x0000000104a5866c chromedriver + 296556\n4   chromedriver                        0x0000000104a5a740 chromedriver + 304960\n5   chromedriver                        0x0000000104acf72c chromedriver + 784172\n6   chromedriver                        0x0000000104acec5c chromedriver + 781404\n7   chromedriver                        0x0000000104a8b004 chromedriver + 503812\n8   chromedriver                        0x0000000104a8b9ec chromedriver + 506348\n9   chromedriver                        0x0000000104df2510 chromedriver + 4072720\n10  chromedriver                        0x0000000104df6fbc chromedriver + 4091836\n11  chromedriver                        0x0000000104dd9754 chromedriver + 3970900\n12  chromedriver                        0x0000000104df78a4 chromedriver + 4094116\n13  chromedriver                        0x0000000104dcc6d4 chromedriver + 3917524\n14  chromedriver                        0x0000000104e14b08 chromedriver + 4213512\n15  chromedriver                        0x0000000104e14c84 chromedriver + 4213892\n16  chromedriver                        0x0000000104e22a08 chromedriver + 4270600\n17  libsystem_pthread.dylib             0x0000000187e3c2e4 _pthread_start + 136\n18  libsystem_pthread.dylib             0x0000000187e370fc thread_start + 8\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStaleElementReferenceException\u001b[0m            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 260\u001b[0m\n\u001b[1;32m    257\u001b[0m     driver\u001b[38;5;241m.\u001b[39mquit()\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 260\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 250\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmain\u001b[39m():\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;66;03m# Step 1: Gather all links (uncomment if you want to always re-collect them):\u001b[39;00m\n\u001b[0;32m--> 250\u001b[0m     \u001b[43mgather_all_links\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;66;03m# Close or refresh the browser if needed (but let's just reuse the same driver)\u001b[39;00m\n\u001b[1;32m    252\u001b[0m     \n\u001b[1;32m    253\u001b[0m     \u001b[38;5;66;03m# Step 2: Scrape based on the links file\u001b[39;00m\n\u001b[1;32m    254\u001b[0m     scrape_from_file()\n",
      "Cell \u001b[0;32mIn[7], line 90\u001b[0m, in \u001b[0;36mgather_all_links\u001b[0;34m()\u001b[0m\n\u001b[1;32m     88\u001b[0m all_links \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 90\u001b[0m     page_links \u001b[38;5;241m=\u001b[39m \u001b[43mcollect_ads_on_page\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m     all_links\u001b[38;5;241m.\u001b[39mextend(page_links)\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m click_next_page():\n",
      "Cell \u001b[0;32mIn[7], line 51\u001b[0m, in \u001b[0;36mcollect_ads_on_page\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m links \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m anchors:\n\u001b[0;32m---> 51\u001b[0m     href \u001b[38;5;241m=\u001b[39m \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_attribute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhref\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m href \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mannons\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m href:\n\u001b[1;32m     53\u001b[0m         links\u001b[38;5;241m.\u001b[39mappend(href)\n",
      "File \u001b[0;32m~/Downloads/ec_utbildning/r_programming/.venv/lib/python3.9/site-packages/selenium/webdriver/remote/webelement.py:231\u001b[0m, in \u001b[0;36mWebElement.get_attribute\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m getAttribute_js \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    230\u001b[0m     _load_js()\n\u001b[0;32m--> 231\u001b[0m attribute_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_script\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/* getAttribute */return (\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mgetAttribute_js\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m).apply(null, arguments);\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m attribute_value\n",
      "File \u001b[0;32m~/Downloads/ec_utbildning/r_programming/.venv/lib/python3.9/site-packages/selenium/webdriver/remote/webdriver.py:528\u001b[0m, in \u001b[0;36mWebDriver.execute_script\u001b[0;34m(self, script, *args)\u001b[0m\n\u001b[1;32m    525\u001b[0m converted_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(args)\n\u001b[1;32m    526\u001b[0m command \u001b[38;5;241m=\u001b[39m Command\u001b[38;5;241m.\u001b[39mW3C_EXECUTE_SCRIPT\n\u001b[0;32m--> 528\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscript\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscript\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43margs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mconverted_args\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Downloads/ec_utbildning/r_programming/.venv/lib/python3.9/site-packages/selenium/webdriver/remote/webdriver.py:429\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    427\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[0;32m--> 429\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    430\u001b[0m     response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/Downloads/ec_utbildning/r_programming/.venv/lib/python3.9/site-packages/selenium/webdriver/remote/errorhandler.py:232\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    230\u001b[0m         alert_text \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[0;32m--> 232\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[0;31mStaleElementReferenceException\u001b[0m: Message: stale element reference: stale element not found in the current frame\n  (Session info: chrome=125.0.6422.141); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#stale-element-reference-exception\nStacktrace:\n0   chromedriver                        0x0000000104e2a4c8 chromedriver + 4302024\n1   chromedriver                        0x0000000104e22e10 chromedriver + 4271632\n2   chromedriver                        0x0000000104a5419c chromedriver + 278940\n3   chromedriver                        0x0000000104a5866c chromedriver + 296556\n4   chromedriver                        0x0000000104a5a740 chromedriver + 304960\n5   chromedriver                        0x0000000104acf72c chromedriver + 784172\n6   chromedriver                        0x0000000104acec5c chromedriver + 781404\n7   chromedriver                        0x0000000104a8b004 chromedriver + 503812\n8   chromedriver                        0x0000000104a8b9ec chromedriver + 506348\n9   chromedriver                        0x0000000104df2510 chromedriver + 4072720\n10  chromedriver                        0x0000000104df6fbc chromedriver + 4091836\n11  chromedriver                        0x0000000104dd9754 chromedriver + 3970900\n12  chromedriver                        0x0000000104df78a4 chromedriver + 4094116\n13  chromedriver                        0x0000000104dcc6d4 chromedriver + 3917524\n14  chromedriver                        0x0000000104e14b08 chromedriver + 4213512\n15  chromedriver                        0x0000000104e14c84 chromedriver + 4213892\n16  chromedriver                        0x0000000104e22a08 chromedriver + 4270600\n17  libsystem_pthread.dylib             0x0000000187e3c2e4 _pthread_start + 136\n18  libsystem_pthread.dylib             0x0000000187e370fc thread_start + 8\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "START_URL = (\n",
    "    \"https://www.blocket.se/bilar/sok?\"\n",
    "    \"filter=%7B\\\"key\\\"%3A\\\"modelYear\\\"%2C\\\"range\\\"%3A%7B\\\"start\\\"%3A\\\"2015\\\"%2C\\\"end\\\"%3A\\\"\\\"%7D%7D&\"\n",
    "    \"filter=%7B\\\"key\\\"%3A\\\"sellerType\\\"%2C\\\"values\\\"%3A%5B\\\"Privat\\\"%5D%7D\"\n",
    ")\n",
    "\n",
    "CARS_FOLDER = \"cars\"\n",
    "LINKS_FILE = \"car_links.txt\"\n",
    "\n",
    "# Path to your ChromeDriver\n",
    "service = ChromeService(executable_path=\"/usr/local/bin/chromedriver\")\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "def bypass_cookies():\n",
    "    \"\"\"Try to click away any cookie banner (if present).\"\"\"\n",
    "    try:\n",
    "        WebDriverWait(driver, 5).until(\n",
    "            EC.frame_to_be_available_and_switch_to_it((By.CSS_SELECTOR, \"iframe[id^='sp_message_iframe_']\"))\n",
    "        )\n",
    "        WebDriverWait(driver, 5).until(\n",
    "            EC.element_to_be_clickable((By.CSS_SELECTOR, \"button.sp_choice_type_11\"))\n",
    "        ).click()\n",
    "        driver.switch_to.default_content()\n",
    "    except:\n",
    "        driver.switch_to.default_content()\n",
    "\n",
    "def sanitize_filename(s):\n",
    "    \"\"\"Clean up strings so they can be used as filenames.\"\"\"\n",
    "    return re.sub(r'[\\\\/:*?\"<>|]+', '_', s)\n",
    "\n",
    "def collect_ads_on_page():\n",
    "    \"\"\"\n",
    "    Collects all 'annons' links on the current page.\n",
    "    \"\"\"\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"div.list.w-full > div > a\"))\n",
    "    )\n",
    "    anchors = driver.find_elements(By.CSS_SELECTOR, \"div.list.w-full > div > a\")\n",
    "    links = []\n",
    "    for a in anchors:\n",
    "        href = a.get_attribute(\"href\")\n",
    "        if href and \"annons\" in href:\n",
    "            links.append(href)\n",
    "    return links\n",
    "\n",
    "def click_next_page():\n",
    "    \"\"\"\n",
    "    Attempts to click the 'Nästa' button (or anchor).  \n",
    "    Returns True if a next page is found and clicked, otherwise False.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Adjust the selector as needed for Blocket. \n",
    "        # This example uses your given CSS path for the next-page button.\n",
    "        next_btn = driver.find_element(\n",
    "            By.CSS_SELECTOR,\n",
    "            \"#__next > main > div.container.mx-auto.mt-3.space-y-4.md\\\\:mt-6 \"\n",
    "            \"> div > div.flex.flex-col.w-full.space-y-6.lg\\\\:col-span-3.md\\\\:items-center \"\n",
    "            \"> div:nth-child(3) > div > div > div:nth-child(2) \"\n",
    "            \"> div.pagination.flex.w-full.justify-between.gap-3.items-stretch.min-h-\\\\[44px\\\\] \"\n",
    "            \"> div.flex.justify-end.flex-1 > a > button\"\n",
    "        )\n",
    "        driver.execute_script(\"arguments[0].click();\", next_btn)\n",
    "        time.sleep(2)  # Give time for next page to load\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def gather_all_links():\n",
    "    \"\"\"\n",
    "    1) Go to START_URL\n",
    "    2) Collect all ad links from each page\n",
    "    3) Paginate until no 'Nästa' is found\n",
    "    4) Writes them into LINKS_FILE\n",
    "    \"\"\"\n",
    "    driver.get(START_URL)\n",
    "    bypass_cookies()\n",
    "\n",
    "    all_links = []\n",
    "    while True:\n",
    "        page_links = collect_ads_on_page()\n",
    "        all_links.extend(page_links)\n",
    "        if not click_next_page():\n",
    "            break\n",
    "    \n",
    "    # Write them to a text file\n",
    "    with open(LINKS_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        for link in all_links:\n",
    "            f.write(link + \"\\n\")\n",
    "    print(f\"[INFO] Wrote {len(all_links)} links to {LINKS_FILE}.\")\n",
    "\n",
    "def extract_price():\n",
    "    \"\"\"Returns only digits (numbers) from the price element, or 'N/A' if not found.\"\"\"\n",
    "    try:\n",
    "        el = driver.find_element(By.CSS_SELECTOR, \"div[class*='Price__StyledPrice']\")\n",
    "        return re.sub(r\"\\D\", \"\", el.text.strip()) or \"N/A\"\n",
    "    except:\n",
    "        return \"N/A\"\n",
    "\n",
    "def get_ad_details():\n",
    "    \"\"\"Wait for the article to appear, then get the title + price.\"\"\"\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.CSS_SELECTOR, \"article\"))\n",
    "    )\n",
    "    try:\n",
    "        title = driver.find_element(By.CSS_SELECTOR, \"h1\").text.strip()\n",
    "    except:\n",
    "        title = \"UnknownTitle\"\n",
    "    return title, extract_price()\n",
    "\n",
    "def save_carousel_images(folder):\n",
    "    \"\"\"\n",
    "    Loop through the image carousel, collect image URLs, and save them.\n",
    "    \"\"\"\n",
    "    CSS_DIVS = \"article div[style*='background-image']\"\n",
    "    CSS_NEXT_BTN = \"button.SliderControls__StyledButton-sc-1dbsnpt-4.cIKvvT\"\n",
    "    all_urls, prev_count = set(), 0\n",
    "\n",
    "    for _ in range(25):  # Arbitrary max slides\n",
    "        divs = driver.find_elements(By.CSS_SELECTOR, CSS_DIVS)\n",
    "        for d in divs:\n",
    "            style = d.get_attribute(\"style\")\n",
    "            m = re.search(r'url\\(\"([^\"]+)\"\\)', style)\n",
    "            if m:\n",
    "                all_urls.add(m.group(1))\n",
    "        if len(all_urls) == prev_count:\n",
    "            break\n",
    "        prev_count = len(all_urls)\n",
    "        try:\n",
    "            btn = driver.find_element(By.CSS_SELECTOR, CSS_NEXT_BTN)\n",
    "            if btn.is_enabled() and btn.is_displayed():\n",
    "                btn.click()\n",
    "                time.sleep(1)\n",
    "            else:\n",
    "                break\n",
    "        except:\n",
    "            break\n",
    "\n",
    "    # Now save them to disk\n",
    "    for i, url in enumerate(all_urls, 1):\n",
    "        img_path = os.path.join(folder, f\"img_{i}.jpg\")\n",
    "        try:\n",
    "            with open(img_path, \"wb\") as f:\n",
    "                f.write(requests.get(url).content)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to save image from {url}: {e}\")\n",
    "\n",
    "def save_parameters(folder):\n",
    "    \"\"\"\n",
    "    Save label-value pairs of ad parameters (e.g. brand, model year, etc.) to separate text files.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        WebDriverWait(driver, 5).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, \"div[class*='ExpandableContent__Content']\"))\n",
    "        )\n",
    "        labels = driver.find_elements(By.CSS_SELECTOR, \"div[class*='ParamsWithIcons__StyledLabel']\")\n",
    "        values = driver.find_elements(By.CSS_SELECTOR, \"div[class*='ParamsWithIcons__StyledParamValue']\")\n",
    "        for i in range(min(len(labels), len(values))):\n",
    "            lbl = labels[i].text.strip() or f\"NoLabel_{i}\"\n",
    "            val = values[i].text.strip() or \"N/A\"\n",
    "            fn = sanitize_filename(lbl)\n",
    "            with open(os.path.join(folder, f\"{fn}.txt\"), \"w\", encoding=\"utf-8\") as ff:\n",
    "                ff.write(val)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "def scrape_single_ad(url):\n",
    "    \"\"\"\n",
    "    Load the ad page, gather info, save everything to disk in CARS_FOLDER.\n",
    "    \"\"\"\n",
    "    driver.get(url)\n",
    "    bypass_cookies()\n",
    "\n",
    "    title, price = get_ad_details()\n",
    "\n",
    "    # Make a folder for this particular ad (limit title length to 80 chars)\n",
    "    folder_name = sanitize_filename(title)[:80] or \"NoTitle\"\n",
    "    folder = os.path.join(CARS_FOLDER, folder_name)\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "    # Save basic info\n",
    "    with open(os.path.join(folder, \"price.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(price)\n",
    "    with open(os.path.join(folder, \"url.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(url)\n",
    "\n",
    "    # Save images\n",
    "    save_carousel_images(folder)\n",
    "    # Save label-value parameters\n",
    "    save_parameters(folder)\n",
    "\n",
    "def scrape_from_file():\n",
    "    \"\"\"\n",
    "    Reads the links from LINKS_FILE and scrapes each ad.\n",
    "    Skips any ad that already has a 'url.txt' saved in its folder.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(LINKS_FILE):\n",
    "        print(f\"[ERROR] File {LINKS_FILE} not found. Run gather_all_links() first.\")\n",
    "        return\n",
    "\n",
    "    with open(LINKS_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        all_links = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "    print(f\"[INFO] Found {len(all_links)} links in {LINKS_FILE}. Starting scrape...\")\n",
    "\n",
    "    os.makedirs(CARS_FOLDER, exist_ok=True)\n",
    "\n",
    "    skipped = 0\n",
    "    scraped = 0\n",
    "\n",
    "    for link in all_links:\n",
    "        try:\n",
    "            # Recreate folder name from ad title preview\n",
    "            driver.get(link)\n",
    "            bypass_cookies()\n",
    "\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, \"h1\"))\n",
    "            )\n",
    "            title = driver.find_element(By.CSS_SELECTOR, \"h1\").text.strip()\n",
    "            folder_name = sanitize_filename(title)[:80] or \"NoTitle\"\n",
    "            folder = os.path.join(CARS_FOLDER, folder_name)\n",
    "\n",
    "            # Skip if folder with url.txt already exists\n",
    "            if os.path.exists(os.path.join(folder, \"url.txt\")):\n",
    "                skipped += 1\n",
    "                print(f\"[SKIP] Already scraped: {folder}\")\n",
    "                continue\n",
    "\n",
    "            scrape_single_ad(link)\n",
    "            scraped += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] {link} -> {e}\")\n",
    "            continue\n",
    "\n",
    "    print(f\"[DONE] Scraped {scraped} new ads, skipped {skipped} existing ones.\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Step 1: Gather all links (uncomment if you want to always re-collect them):\n",
    "    gather_all_links()\n",
    "    # Close or refresh the browser if needed (but let's just reuse the same driver)\n",
    "    \n",
    "    # Step 2: Scrape based on the links file\n",
    "    scrape_from_file()\n",
    "\n",
    "    # Done\n",
    "    driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Found 6111 links in car_links.txt. Starting scrape...\n",
      "[SKIP] Already scraped: cars/Toyota Prius+ Hybrid CVT Euro 6\n",
      "[SKIP] Already scraped: cars/Audi A6 Avant 2.0 TDI clean diesel quattro S Tronic Ambition\n",
      "[SKIP] Already scraped: cars/Nissan Qashqai 1.5 dCi Euro 6\n",
      "[SKIP] Already scraped: cars/Volkswagen ID.3 Max\n",
      "[SKIP] Already scraped: cars/Chevrolet Camaro 3.6 V6 Hydra-Matic\n",
      "[SKIP] Already scraped: cars/Tesla Model 3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "START_URL = (\n",
    "    \"https://www.blocket.se/bilar/sok?\"\n",
    "    \"filter=%7B\\\"key\\\"%3A\\\"modelYear\\\"%2C\\\"range\\\"%3A%7B\\\"start\\\"%3A\\\"2015\\\"%2C\\\"end\\\"%3A\\\"\\\"%7D%7D&\"\n",
    "    \"filter=%7B\\\"key\\\"%3A\\\"sellerType\\\"%2C\\\"values\\\"%3A%5B\\\"Privat\\\"%5D%7D\"\n",
    ")\n",
    "\n",
    "CARS_FOLDER = \"cars\"\n",
    "LINKS_FILE = \"car_links.txt\"\n",
    "\n",
    "service = ChromeService(executable_path=\"/usr/local/bin/chromedriver\")\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "def bypass_cookies():\n",
    "    \"\"\"Try to click away any cookie banner (if present).\"\"\"\n",
    "    try:\n",
    "        WebDriverWait(driver, 5).until(\n",
    "            EC.frame_to_be_available_and_switch_to_it((By.CSS_SELECTOR, \"iframe[id^='sp_message_iframe_']\"))\n",
    "        )\n",
    "        WebDriverWait(driver, 5).until(\n",
    "            EC.element_to_be_clickable((By.CSS_SELECTOR, \"button.sp_choice_type_11\"))\n",
    "        ).click()\n",
    "        driver.switch_to.default_content()\n",
    "    except:\n",
    "        driver.switch_to.default_content()\n",
    "\n",
    "def sanitize_filename(s):\n",
    "    \"\"\"Clean up strings so they can be used as filenames.\"\"\"\n",
    "    return re.sub(r'[\\\\/:*?\"<>|]+', '_', s)\n",
    "\n",
    "def extract_price():\n",
    "    \"\"\"Returns only digits from the price element, or 'N/A' if not found.\"\"\"\n",
    "    try:\n",
    "        el = driver.find_element(By.CSS_SELECTOR, \"div[class*='Price__StyledPrice']\")\n",
    "        return re.sub(r\"\\D\", \"\", el.text.strip()) or \"N/A\"\n",
    "    except:\n",
    "        return \"N/A\"\n",
    "\n",
    "def get_ad_details():\n",
    "    \"\"\"Wait for the article to appear, then get the title and price.\"\"\"\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.CSS_SELECTOR, \"article\"))\n",
    "    )\n",
    "    try:\n",
    "        title = driver.find_element(By.CSS_SELECTOR, \"h1\").text.strip()\n",
    "    except:\n",
    "        title = \"UnknownTitle\"\n",
    "    return title, extract_price()\n",
    "\n",
    "def save_carousel_images(folder):\n",
    "    \"\"\"Loop through the image carousel, collect image URLs, and save them.\"\"\"\n",
    "    CSS_DIVS = \"article div[style*='background-image']\"\n",
    "    CSS_NEXT_BTN = \"button.SliderControls__StyledButton-sc-1dbsnpt-4.cIKvvT\"\n",
    "    all_urls, prev_count = set(), 0\n",
    "\n",
    "    for _ in range(25):  # arbitrary max slides\n",
    "        divs = driver.find_elements(By.CSS_SELECTOR, CSS_DIVS)\n",
    "        for d in divs:\n",
    "            style = d.get_attribute(\"style\")\n",
    "            m = re.search(r'url\\(\"([^\"]+)\"\\)', style)\n",
    "            if m:\n",
    "                all_urls.add(m.group(1))\n",
    "        if len(all_urls) == prev_count:\n",
    "            break\n",
    "        prev_count = len(all_urls)\n",
    "        try:\n",
    "            btn = driver.find_element(By.CSS_SELECTOR, CSS_NEXT_BTN)\n",
    "            if btn.is_enabled() and btn.is_displayed():\n",
    "                btn.click()\n",
    "                time.sleep(1)\n",
    "            else:\n",
    "                break\n",
    "        except:\n",
    "            break\n",
    "\n",
    "    # Save them to disk\n",
    "    for i, url in enumerate(all_urls, 1):\n",
    "        img_path = os.path.join(folder, f\"img_{i}.jpg\")\n",
    "        try:\n",
    "            with open(img_path, \"wb\") as f:\n",
    "                f.write(requests.get(url).content)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to save image from {url}: {e}\")\n",
    "\n",
    "def save_parameters(folder):\n",
    "    \"\"\"Save label-value pairs of ad parameters to separate text files.\"\"\"\n",
    "    try:\n",
    "        WebDriverWait(driver, 5).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, \"div[class*='ExpandableContent__Content']\"))\n",
    "        )\n",
    "        labels = driver.find_elements(By.CSS_SELECTOR, \"div[class*='ParamsWithIcons__StyledLabel']\")\n",
    "        values = driver.find_elements(By.CSS_SELECTOR, \"div[class*='ParamsWithIcons__StyledParamValue']\")\n",
    "        for i in range(min(len(labels), len(values))):\n",
    "            lbl = labels[i].text.strip() or f\"NoLabel_{i}\"\n",
    "            val = values[i].text.strip() or \"N/A\"\n",
    "            fn = sanitize_filename(lbl)\n",
    "            with open(os.path.join(folder, f\"{fn}.txt\"), \"w\", encoding=\"utf-8\") as ff:\n",
    "                ff.write(val)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "def scrape_single_ad(url):\n",
    "    \"\"\"\n",
    "    Load the ad page, gather info, save everything to disk in CARS_FOLDER.\n",
    "    \"\"\"\n",
    "    driver.get(url)\n",
    "    bypass_cookies()\n",
    "\n",
    "    title, price = get_ad_details()\n",
    "\n",
    "    folder_name = sanitize_filename(title)[:80] or \"NoTitle\"\n",
    "    folder = os.path.join(CARS_FOLDER, folder_name)\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "    # Save basic info\n",
    "    with open(os.path.join(folder, \"price.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(price)\n",
    "    with open(os.path.join(folder, \"url.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(url)\n",
    "\n",
    "    # Save images\n",
    "    save_carousel_images(folder)\n",
    "    # Save label-value parameters\n",
    "    save_parameters(folder)\n",
    "\n",
    "def scrape_from_file():\n",
    "    \"\"\"\n",
    "    Reads the links from LINKS_FILE and scrapes each ad, skipping any ad that's\n",
    "    already stored (i.e., the folder with url.txt already exists).\n",
    "    \"\"\"\n",
    "    if not os.path.exists(LINKS_FILE):\n",
    "        print(f\"[ERROR] File {LINKS_FILE} not found. Please provide a valid file.\")\n",
    "        return\n",
    "\n",
    "    with open(LINKS_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        all_links = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "    print(f\"[INFO] Found {len(all_links)} links in {LINKS_FILE}. Starting scrape...\")\n",
    "    os.makedirs(CARS_FOLDER, exist_ok=True)\n",
    "\n",
    "    skipped = 0\n",
    "    scraped = 0\n",
    "\n",
    "    for link in all_links:\n",
    "        try:\n",
    "            # Peek at the ad title (to know folder name) before deciding if we skip\n",
    "            driver.get(link)\n",
    "            bypass_cookies()\n",
    "\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, \"h1\"))\n",
    "            )\n",
    "            title = driver.find_element(By.CSS_SELECTOR, \"h1\").text.strip()\n",
    "            folder_name = sanitize_filename(title)[:80] or \"NoTitle\"\n",
    "            folder = os.path.join(CARS_FOLDER, folder_name)\n",
    "\n",
    "            # Skip if folder with url.txt already exists\n",
    "            if os.path.exists(os.path.join(folder, \"url.txt\")):\n",
    "                skipped += 1\n",
    "                print(f\"[SKIP] Already scraped: {folder}\")\n",
    "                continue\n",
    "\n",
    "            # Otherwise, do the full scrape\n",
    "            scrape_single_ad(link)\n",
    "            scraped += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] {link} -> {e}\")\n",
    "            continue\n",
    "\n",
    "    print(f\"[DONE] Scraped {scraped} new ads, skipped {skipped} existing ones.\")\n",
    "\n",
    "def main():\n",
    "    # Instead of gathering links, we directly call scrape_from_file.\n",
    "    scrape_from_file()\n",
    "    driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Removed 0 directories. 813 directories left.\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "def remove_dirs_with_slap(cars_folder=\"cars\"):\n",
    "    removed_count = 0\n",
    "    total_count = 0\n",
    "    for root, dirs, files in os.walk(cars_folder):\n",
    "        if \"Biltyp.txt\" in files:\n",
    "            total_count += 1\n",
    "            with open(os.path.join(root, \"Biltyp.txt\"), \"r\", encoding=\"utf-8\") as f:\n",
    "                content = f.read()\n",
    "                if \"Släp\" in content or \"Slap\" in content or \"Husvagn\" in content or \"Vagn\" in content or \"Husbil\" in content:\n",
    "                    print(f\"[INFO] Removing directory: {root}\")\n",
    "                    shutil.rmtree(root)\n",
    "                    removed_count += 1\n",
    "    print(f\"[INFO] Removed {removed_count} directories. {total_count - removed_count} directories left.\")\n",
    "\n",
    "remove_dirs_with_slap()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def create_already_matched_file(cars_folder=\"cars\"):\n",
    "    already_matched_links = []\n",
    "    for root, dirs, files in os.walk(cars_folder):\n",
    "        if \"url.txt\" in files:\n",
    "            with open(os.path.join(root, \"url.txt\"), \"r\", encoding=\"utf-8\") as f:\n",
    "                links = f.readlines()\n",
    "                already_matched_links.extend([link.strip() for link in links])\n",
    "\n",
    "    with open(\"already_matched.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for link in already_matched_links:\n",
    "            f.write(f\"{link}\\n\")\n",
    "\n",
    "create_already_matched_file()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
