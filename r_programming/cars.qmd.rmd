---
title: "Blocket Car Analysis"
author:
  - Geisol
  - Maria
format: html
---

# Introduction

In this project, we analyze web-scraped used car listings from Blocket, enriched with technical specifications from Transportstyrelsen and cost estimates for insurance and taxation in Sweden. The primary goal has shifted from estimating total yearly ownership cost to two main objectives:
(1) Predicting market prices for used cars in Sweden, with a focus on non-premium, mid-range vehicles, and
(2) Gaining statistical insight into what factors influence insurance premiums and vehicle taxes, using linear regression models.

By combining predictive modeling (e.g., random forest) with inference-driven approaches (e.g., linear regression), we aim to both improve price prediction accuracy and interpret underlying patterns in insurance and tax structures in the Swedish car market.


---

```{r}
#| label: setup
#| message: false
#| warning: false
#| results: hide      # <‑ add this if you want zero printed output

packages <- c(
  "tidyverse", "readxl", "fastDummies", "corrplot",
  "rsample", "GGally", "ggcorrplot", "car", "xgboost", "yardstick", "tibble", "ranger", "glmnet", "vip"
)

idx <- packages %in% rownames(installed.packages())
if (any(!idx)) install.packages(packages[!idx])

suppressPackageStartupMessages(
  invisible(lapply(packages, library, character.only = TRUE))
)
```



```{r}
#| label: import-and-clean
#| message: false
#| warning: false

# Read the data (adjust file name/path if needed) -------------------------
cars_raw <- read_excel("dataset_final.xlsx")

# 1. Convert character columns that look numeric into numeric --------------
cars_clean <- cars_raw %>%
  mutate(across(
    where(is.character),
    ~ ifelse(
        grepl("^[0-9.,]+$", .x),
        suppressWarnings(as.numeric(gsub(",", "", .x))),
        .x
      )
  )) %>%
  # 2. Convert energy consumption to numeric and replace NAs with 0 --------
  mutate(`Energiförbrukning (Wh/km)` = replace_na(as.numeric(`Energiförbrukning (Wh/km)`), 0)) %>%
  # 3. Convert Modellår to factor ------------------------------------------
  mutate(Modellår = factor(Modellår)) %>%
  # 4. Remove Biltyp completely --------------------------------------------
  select(-Biltyp)

glimpse(cars_clean)

```


#Distribution inspection

```{r}
#| label: distribution-check
#| message: false
#| warning: false

# Visualise distributions of all numeric variables -----------------------
numeric_vars <- cars_clean %>%
  select(where(is.numeric)) %>%
  select(where(~ n_distinct(.) > 2)) %>%
  pivot_longer(everything(), names_to = "Variable", values_to = "Value")

ggplot(numeric_vars, aes(Value)) +
  geom_histogram(bins = 30, fill = "steelblue", color = "white") +
  facet_wrap(~ Variable, scales = "free") +
  theme_minimal() +
  labs(x = NULL, y = "Count")

```

```{r}
#| label: modeling-dataset
#| message: false
#| warning: false

# Final modelling dataset -------------------------------------------------
cars_clean_inf <- cars_clean %>%
  mutate(
    Helförsäkring_log = log(`Helförsäkring (kr / år)`),
    Hästkrafter_log        = log(Hästkrafter + 1),
    Pris_log          = log(`Pris (kr)`),
    Skatt_log         = log(`Fordonsskatt (kr / år)` + 1),
    Mätar_log         = log(`Mätarställning (km)` + 1),
    Bränsleförb_log   = log(`Bränsleförbrukning (l / 100 km)` + 1)
  ) %>%
  select(
    Helförsäkring_log, Hästkrafter_log, Modellår, Mätar_log,
    Bränsleförb_log, Bränsle, Pris_log, Skatt_log, `Koldioxidutsläpp blandad (NEDC) g/km`
  )

glimpse(cars_clean_inf)
```

## Correlation inspection

```{r}
#| label: correlation
#| message: false
#| warning: false

cars_clean_inf %>%
  select(where(is.numeric)) %>%
  summarise(across(everything(), ~ cor(.x, cars_clean_inf$Helförsäkring_log, use = "complete.obs"))) %>%
  pivot_longer(everything(), names_to = "Variable", values_to = "Correlation") %>%
  arrange(desc(abs(Correlation)))
```


# Statistical Inference

```{r}
#| label: inference-models-försäkring
#| message: false
#| warning: false

model_f <- lm(Helförsäkring_log ~ ., data = cars_clean_inf%>%select(-Bränsleförb_log, -Skatt_log, -`Koldioxidutsläpp blandad (NEDC) g/km`))
summary(model_f)
confint(model_f)

# Diagnostics -------------------------------------------------------------
par(mfrow = c(2, 2))
plot(model_f)
par(mfrow = c(1, 1))

vif(model_f)

```

```{r}
#| label: inference-models-försäkring
#| message: false
#| warning: false

model_p <- lm(Pris_log ~ .,  data = cars_clean_inf %>% select(-Helförsäkring_log, -Skatt_log, -Bränsleförb_log, -`Koldioxidutsläpp blandad (NEDC) g/km`))

summary(model_p)
confint(model_p)

# Diagnostics -------------------------------------------------------------
par(mfrow = c(2, 2))
plot(model_p)
par(mfrow = c(1, 1))

vif(model_p)

```

```{r}
#| label: inference-models-skatt
#| message: false
#| warning: false

model_s <- lm(Skatt_log ~ .,  data = cars_clean_inf %>% select(-Helförsäkring_log, -Bränsleförb_log))

summary(model_s)
confint(model_s)

# Diagnostics -------------------------------------------------------------
par(mfrow = c(2, 2))
plot(model_s)
par(mfrow = c(1, 1))

vif(model_s)

```




---

# Create a new dataset
```{r}
#| label: Create a new dataset
#| message: false
#| warning: false


# 0) Select variables, log-transform, dummy code, and clean names
cars_price <- cars_clean %>%
  select(
    pris                 = `Pris (kr)`,
    fordonsskatt         = `Fordonsskatt (kr / år)`,
    helforsakring        = `Helförsäkring (kr / år)`,
    hastkrafter          = Hästkrafter,
    matarstallning       = `Mätarställning (km)`,
    co2                  = `Koldioxidutsläpp blandad (NEDC) g/km`
  ) %>%
  mutate(
    pris_log             = log(pris),
    fordonsskatt_log     = log(fordonsskatt + 1),
    helforsakring_log    = log(helforsakring + 1),
    hastkrafter_log      = log(hastkrafter + 1),
    matarstallning_log   = log(matarstallning + 1),
  ) %>%
  select(
    pris,
    ends_with("_log")
  ) %>%
  clean_names()
# Kontrollera strukturen
glimpse(cars_price)

```

# Predictive Modeling

```{r}
#| label: rf-price-prediction
#| message: false
#| warning: false

library(dplyr)
library(fastDummies)
library(janitor)
library(rsample)
library(ranger)
library(purrr)
library(yardstick)
library(tibble)

# Define the log-transformed numeric columns
log_vars <- c(
  "pris_log", 
  "fordonsskatt_log", 
  "helforsakring_log",
  "hastkrafter_log",
  "matarstallning_log"
)

# 0) Beräkna outliergränser före split ------------------------
compute_outlier_bounds <- function(df, columns, lower = 0.01, upper = 0.99) {
  map(columns, function(col) {
    q_low  <- quantile(df[[col]], lower, na.rm = TRUE)
    q_high <- quantile(df[[col]], upper, na.rm = TRUE)
    tibble(variable = col, q_low = q_low, q_high = q_high)
  }) %>% bind_rows()
}

outlier_bounds <- compute_outlier_bounds(cars_price, log_vars)

# Funktion för att filtrera bort outliers enligt dessa gränser ------------
apply_outlier_filter <- function(df, bounds) {
  for (i in seq_len(nrow(bounds))) {
    col <- bounds$variable[i]
    df <- df %>% filter(.data[[col]] >= bounds$q_low[i], .data[[col]] <= bounds$q_high[i])
  }
  df
}



# 1) Train/test split (80/20), stratified on log(price)
set.seed(123)
split <- initial_split(cars_price, prop = 0.8, strata = pris_log)
train <- training(split)
test  <- testing(split)

# 1) Split
set.seed(123)
split <- initial_split(cars_price, prop = 0.8, strata = pris_log)
train <- training(split)
test  <- testing(split)

# 2) Rensa outliers i båda
train <- apply_outlier_filter(train, outlier_bounds)
test  <- apply_outlier_filter(test,  outlier_bounds)



# 3) Create 5-fold stratified CV on cleaned training set
set.seed(123)
folds <- vfold_cv(train, v = 5, strata = pris_log)

# 4) Light manual tuning of hyperparameters
param_grid <- expand.grid(
  num.trees     = c(50, 200),
  mtry          = c(floor(ncol(train)/2)),
  min.node.size = c(1, 10),
  max.depth     = c(2, 12)
)

cv_results <- map_dfr(1:nrow(param_grid), function(i) {
  p <- param_grid[i, ]
  
  fold_metrics <- map_dfr(folds$splits, function(split) {
    tr <- analysis(split)
    va <- assessment(split)
    
    mod <- ranger(
      formula       = pris_log ~ .,
      data          = tr %>% select(-pris),
      num.trees     = p$num.trees,
      mtry          = p$mtry,
      min.node.size = p$min.node.size,
      max.depth     = p$max.depth
    )
    
    pred_log <- predict(mod, va)$predictions
    pred_original <- exp(pred_log)
    
    tibble(
      RMSE_log = rmse_vec(truth = va$pris_log, estimate = pred_log),
      RMSE_SEK = rmse_vec(truth = va$pris, estimate = pred_original)
    )
  })
  
  tibble(
    num.trees     = p$num.trees,
    mtry          = p$mtry,
    min.node.size = p$min.node.size,
    max.depth     = p$max.depth,
    RMSE_log      = mean(fold_metrics$RMSE_log),
    RMSE_SEK      = mean(fold_metrics$RMSE_SEK)
  )
})

# 5) Choose best hyperparameter set
best_params <- cv_results %>%
  arrange(RMSE_log) %>%
  dplyr::slice(1)


# 6) Train final model on full training set with best parameters
final_model <- ranger(
  formula       = pris_log ~ .,
  data          = train %>% select(-pris),
  num.trees     = best_params$num.trees,
  mtry          = best_params$mtry,
  min.node.size = best_params$min.node.size,
  max.depth     = best_params$max.depth,
  importance    = "impurity"
)


# 7) Predict on test set and back-transform
pred_log_test <- predict(final_model, data = test)$predictions
pred_original_test <- exp(pred_log_test)

# 8) Compute RMSE on test set (log and original scale)
rmse_log_test <- rmse_vec(truth = test$pris_log, estimate = pred_log_test)
rmse_original_test <- rmse_vec(truth = test$pris, estimate = pred_original_test)

# 9) Output final results
cat("\nBest Hyperparameters:\n")
print(best_params)

cat("\nTest Set Results:\n")
print(tibble(
  Dataset   = "Test Set (Final)",
  RMSE_log  = round(rmse_log_test, 4),
  RMSE_SEK  = round(rmse_original_test, 2)
))


library(vip)  # Variable Importance Plots

vip(final_model, num_features = 10, bar = TRUE) +
  ggtitle("Variable Importance (Predictors)")

```



